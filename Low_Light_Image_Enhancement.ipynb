{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sxzs6SFHkhWJ",
        "outputId": "6cffa327-a238-492d-fe7d-9763a20eacbc"
      },
      "outputs": [],
      "source": [
        "!pip install tensorflow tensorflow_addons --quiet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ARHa4TtCkh4S"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_addons as tfa"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FfoJo-p7w0TG"
      },
      "outputs": [],
      "source": [
        "!pip install transformers --upgrade --quiet\n",
        "!pip install git+https://github.com/wandb/wandb.git\n",
        "!yes | apt install --allow-change-held-packages libcudnn8=8.1.0.77-1+cuda11.2\n",
        "\n",
        "\n",
        "import os\n",
        "\n",
        "\n",
        "\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
        "os.environ['WANDB_API_KEY'] = 'd01b92fa0118a7bba3d1f90cfda3e30f3addf78f'\n",
        "\n",
        "\n",
        "import random\n",
        "import numpy as np\n",
        "from glob import glob\n",
        "from functools import partial\n",
        "from PIL import Image, ImageOps\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from mpl_toolkits.axes_grid1 import ImageGrid\n",
        "\n",
        "from transformers.tf_utils import shape_list\n",
        "\n",
        "import wandb\n",
        "from wandb.keras import WandbMetricsLogger, WandbModelCheckpoint\n",
        "\n",
        "\n",
        "AUTOTUNE = tf.data.AUTOTUNE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "pOutp-Uckjm3"
      },
      "outputs": [],
      "source": [
        "wandb.init(project=\"mirnet-v2\",entity=\"nemesis_xvi\", job_type=\"colab-train\")\n",
        "\n",
        "strategy = tf.distribute.MirroredStrategy()\n",
        "\n",
        "config = wandb.config\n",
        "\n",
        "config.seed = 42\n",
        "random.seed(config.seed)\n",
        "tf.random.set_seed(config.seed)\n",
        "\n",
        "config.num_gpus = len(tf.config.list_physical_devices('GPU'))\n",
        "config.dataset_artifact_address = 'nemesis_xvi/mirnet-v2/lol-dataset:v0'\n",
        "config.image_size = 128\n",
        "config.max_train_images = 400\n",
        "config.batch_size_per_replica = 4\n",
        "config.batch_size = config.batch_size_per_replica * strategy.num_replicas_in_sync\n",
        "\n",
        "config.channels = 80\n",
        "config.channel_factor = 1.5\n",
        "config.num_mrb_blocks = 2\n",
        "config.add_residual_connection = True\n",
        "\n",
        "config.initial_learning_rate = 2e-4\n",
        "config.minimum_learning_rate = 1e-6\n",
        "config.epochs = 100"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YM-VFQj3ktOv"
      },
      "outputs": [],
      "source": [
        "!mkdir LOLdataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ORxvHdW_kmzP"
      },
      "outputs": [],
      "source": [
        "!unzip '/content/drive/MyDrive/LOLdataset.zip' -d '/content/LOLdataset'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RZ_9LtAnk4ZS"
      },
      "outputs": [],
      "source": [
        "dataset_dir = \"/content/LOLdataset\"\n",
        "TRAIN_LOW_LIGHT_IMAGES = sorted(\n",
        "    glob(os.path.join(dataset_dir, \"our485/low/*\"))\n",
        ")[:config.max_train_images]\n",
        "TRAIN_GROUND_TRUTH_IMAGES = sorted(\n",
        "    glob(os.path.join(dataset_dir, \"our485/high/*\"))\n",
        ")[:config.max_train_images]\n",
        "\n",
        "VAL_LOW_LIGHT_IMAGES = sorted(\n",
        "    glob(os.path.join(dataset_dir, \"our485/low/*\"))\n",
        ")[config.max_train_images:]\n",
        "VAL_GROUND_TRUTH_IMAGES = sorted(\n",
        "    glob(os.path.join(dataset_dir, \"our485/high/*\"))\n",
        ")[config.max_train_images:]\n",
        "\n",
        "TEST_LOW_LIGHT_IMAGES = sorted(\n",
        "    glob(os.path.join(dataset_dir, \"eval15/low/*\"))\n",
        ")\n",
        "TEST_GROUND_TRUTH_IMAGES = sorted(\n",
        "    glob(os.path.join(dataset_dir, \"eval15/high/*\"))\n",
        ")\n",
        "\n",
        "print(f\"Train Dataset: ({len(TRAIN_LOW_LIGHT_IMAGES)}, {len(TRAIN_GROUND_TRUTH_IMAGES)})\")\n",
        "print(f\"Validation Dataset: ({len(VAL_LOW_LIGHT_IMAGES)}, {len(VAL_GROUND_TRUTH_IMAGES)})\")\n",
        "print(f\"Test Dataset: ({len(TEST_LOW_LIGHT_IMAGES)}, {len(TEST_GROUND_TRUTH_IMAGES)})\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wGoDY4U-k4wD"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_kg_hide-output": true,
        "execution": {
          "iopub.execute_input": "2022-12-06T02:49:58.845845Z",
          "iopub.status.busy": "2022-12-06T02:49:58.845486Z",
          "iopub.status.idle": "2022-12-06T02:49:58.864188Z",
          "shell.execute_reply": "2022-12-06T02:49:58.863272Z"
        },
        "id": "9b4580f9",
        "papermill": {
          "duration": 0.061143,
          "end_time": "2022-12-06T02:49:58.866882",
          "exception": false,
          "start_time": "2022-12-06T02:49:58.805739",
          "status": "completed"
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "def read_image(image_path):\n",
        "    image = tf.io.read_file(image_path)\n",
        "    image = tf.image.decode_png(image, channels=3)\n",
        "    image = tf.cast(image, dtype=tf.float32) / 255.0\n",
        "    return image\n",
        "\n",
        "\n",
        "def random_crop(low_image, gt_image):\n",
        "    low_image_shape = tf.shape(low_image)[:2]\n",
        "    crop_width = tf.random.uniform(\n",
        "        shape=(), maxval=low_image_shape[1] - config.image_size + 1, dtype=tf.int32\n",
        "    )\n",
        "    crop_height = tf.random.uniform(\n",
        "        shape=(), maxval=low_image_shape[0] - config.image_size + 1, dtype=tf.int32\n",
        "    )\n",
        "    low_image_cropped = low_image[\n",
        "        crop_height : crop_height + config.image_size,\n",
        "        crop_width : crop_width + config.image_size\n",
        "    ]\n",
        "    gt_image_cropped = gt_image[\n",
        "        crop_height : crop_height + config.image_size,\n",
        "        crop_width : crop_width + config.image_size\n",
        "    ]\n",
        "    low_image_cropped.set_shape([config.image_size, config.image_size, 3])\n",
        "    gt_image_cropped.set_shape([config.image_size, config.image_size, 3])\n",
        "    return low_image_cropped, gt_image_cropped\n",
        "\n",
        "\n",
        "def resize_images(low_image, gt_image):\n",
        "    low_image = tf.image.resize(low_image, size=[config.image_size, config.image_size])\n",
        "    gt_image = tf.image.resize(gt_image, size=[config.image_size, config.image_size])\n",
        "    low_image.set_shape([config.image_size, config.image_size, 3])\n",
        "    gt_image.set_shape([config.image_size, config.image_size, 3])\n",
        "    return low_image, gt_image\n",
        "\n",
        "\n",
        "def load_data(low_light_image_path, enhanced_image_path, apply_resize):\n",
        "    low_light_image = read_image(low_light_image_path)\n",
        "    enhanced_image = read_image(enhanced_image_path)\n",
        "    low_light_image, enhanced_image = (\n",
        "        resize_images(low_light_image, enhanced_image) if apply_resize\n",
        "        else random_crop(low_light_image, enhanced_image)\n",
        "    )\n",
        "    return low_light_image, enhanced_image\n",
        "\n",
        "\n",
        "def get_dataset(low_light_images, enhanced_images, apply_resize):\n",
        "    dataset = tf.data.Dataset.from_tensor_slices((low_light_images, enhanced_images))\n",
        "    dataset = dataset.map(\n",
        "        partial(load_data, apply_resize=apply_resize),\n",
        "        num_parallel_calls=AUTOTUNE\n",
        "    )\n",
        "    dataset = dataset.batch(config.batch_size, drop_remainder=True)\n",
        "    dataset = dataset.prefetch(AUTOTUNE)\n",
        "    return dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e46CF-zhk9CP"
      },
      "outputs": [],
      "source": [
        "train_dataset = get_dataset(TRAIN_LOW_LIGHT_IMAGES, TRAIN_GROUND_TRUTH_IMAGES, apply_resize=False)\n",
        "val_dataset = get_dataset(VAL_LOW_LIGHT_IMAGES, VAL_GROUND_TRUTH_IMAGES, apply_resize=True)\n",
        "\n",
        "print(\"Train Dataset:\", train_dataset.element_spec)\n",
        "print(\"Validation Dataset:\", val_dataset.element_spec)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1LMU0arxk_cr"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-12-06T02:49:59.965376Z",
          "iopub.status.busy": "2022-12-06T02:49:59.964942Z",
          "iopub.status.idle": "2022-12-06T02:49:59.977824Z",
          "shell.execute_reply": "2022-12-06T02:49:59.976931Z"
        },
        "id": "dc7c8399",
        "papermill": {
          "duration": 0.063307,
          "end_time": "2022-12-06T02:49:59.980331",
          "exception": false,
          "start_time": "2022-12-06T02:49:59.917024",
          "status": "completed"
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "class SelectiveKernelFeatureFusion(tf.keras.layers.Layer):\n",
        "    def __init__(self, channels: int, *args, **kwargs) -> None:\n",
        "        super().__init__(*args, **kwargs)\n",
        "\n",
        "        self.hidden_channels = max(int(channels / 8), 4)\n",
        "\n",
        "        self.average_pooling = tfa.layers.AdaptiveAveragePooling2D(output_size=1)\n",
        "\n",
        "        self.conv_channel_downscale = tf.keras.layers.Conv2D(\n",
        "            self.hidden_channels, kernel_size=1, padding=\"same\"\n",
        "        )\n",
        "        self.conv_attention_1 = tf.keras.layers.Conv2D(\n",
        "            channels, kernel_size=1, strides=1, padding=\"same\"\n",
        "        )\n",
        "        self.conv_attention_2 = tf.keras.layers.Conv2D(\n",
        "            channels, kernel_size=1, strides=1, padding=\"same\"\n",
        "        )\n",
        "\n",
        "        self.leaky_relu = tf.keras.layers.LeakyReLU(alpha=0.2)\n",
        "        self.softmax = tf.keras.layers.Softmax(axis=-1)\n",
        "\n",
        "    def call(self, inputs, *args, **kwargs):\n",
        "        combined_input_features = inputs[0] + inputs[1]\n",
        "        channel_wise_statistics = self.average_pooling(combined_input_features)\n",
        "        downscaled_channel_wise_statistics = self.conv_channel_downscale(\n",
        "            channel_wise_statistics\n",
        "        )\n",
        "        attention_vector_1 = self.softmax(\n",
        "            self.conv_attention_1(downscaled_channel_wise_statistics)\n",
        "        )\n",
        "        attention_vector_2 = self.softmax(\n",
        "            self.conv_attention_2(downscaled_channel_wise_statistics)\n",
        "        )\n",
        "        selected_features = (\n",
        "            inputs[0] * attention_vector_1 + inputs[1] * attention_vector_2\n",
        "        )\n",
        "        return selected_features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4pCWwJGGlCI4"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-12-06T02:50:00.142636Z",
          "iopub.status.busy": "2022-12-06T02:50:00.142183Z",
          "iopub.status.idle": "2022-12-06T02:50:00.161579Z",
          "shell.execute_reply": "2022-12-06T02:50:00.160675Z"
        },
        "id": "4d2d79c7",
        "papermill": {
          "duration": 0.061416,
          "end_time": "2022-12-06T02:50:00.163922",
          "exception": false,
          "start_time": "2022-12-06T02:50:00.102506",
          "status": "completed"
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "class ContextBlock(tf.keras.layers.Layer):\n",
        "    def __init__(self, channels: int, *args, **kwargs):\n",
        "        super().__init__(*args, **kwargs)\n",
        "\n",
        "        self.mask_conv = tf.keras.layers.Conv2D(\n",
        "            1, kernel_size=1, padding=\"same\"\n",
        "        )\n",
        "\n",
        "        self.channel_add_conv_1 = tf.keras.layers.Conv2D(\n",
        "            channels, kernel_size=1, padding=\"same\"\n",
        "        )\n",
        "        self.channel_add_conv_2 = tf.keras.layers.Conv2D(\n",
        "            channels, kernel_size=1, padding=\"same\"\n",
        "        )\n",
        "\n",
        "        self.softmax = tf.keras.layers.Softmax(axis=1)\n",
        "        self.leaky_relu = tf.keras.layers.LeakyReLU(alpha=0.2)\n",
        "\n",
        "    def modeling(self, inputs):\n",
        "        _, height, width, channels = shape_list(inputs)\n",
        "        reshaped_inputs = tf.expand_dims(\n",
        "            tf.reshape(inputs, (-1, channels, height * width)), axis=1\n",
        "        )\n",
        "\n",
        "        context_mask = self.mask_conv(inputs)\n",
        "        context_mask = tf.reshape(context_mask, (-1, height * width, 1))\n",
        "        context_mask = self.softmax(context_mask)\n",
        "        context_mask = tf.expand_dims(context_mask, axis=1)\n",
        "\n",
        "        context = tf.reshape(\n",
        "            tf.matmul(reshaped_inputs, context_mask), (-1, 1, 1, channels)\n",
        "        )\n",
        "        return context\n",
        "\n",
        "    def call(self, inputs, *args, **kwargs):\n",
        "        context = self.modeling(inputs)\n",
        "        channel_add_term = self.channel_add_conv_1(context)\n",
        "        channel_add_term = self.leaky_relu(channel_add_term)\n",
        "        channel_add_term = self.channel_add_conv_2(channel_add_term)\n",
        "        return inputs + channel_add_term\n",
        "\n",
        "\n",
        "class ResidualContextBlock(tf.keras.layers.Layer):\n",
        "    def __init__(self, channels: int, groups: int, *args, **kwargs):\n",
        "        super().__init__(*args, **kwargs)\n",
        "\n",
        "        self.conv_1 = tf.keras.layers.Conv2D(\n",
        "            channels, kernel_size=3, padding=\"same\", groups=groups\n",
        "        )\n",
        "        self.conv_2 = tf.keras.layers.Conv2D(\n",
        "            channels, kernel_size=3, padding=\"same\", groups=groups\n",
        "        )\n",
        "        self.leaky_relu = tf.keras.layers.LeakyReLU(alpha=0.2)\n",
        "\n",
        "        self.context_block = ContextBlock(channels=channels)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        x = self.conv_1(inputs)\n",
        "        x = self.leaky_relu(x)\n",
        "        x = self.conv_2(x)\n",
        "        x = self.context_block(x)\n",
        "        x = self.leaky_relu(x)\n",
        "        x = x + inputs\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2WvDhgYRlEhE"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-12-06T02:50:00.329594Z",
          "iopub.status.busy": "2022-12-06T02:50:00.329121Z",
          "iopub.status.idle": "2022-12-06T02:50:00.343934Z",
          "shell.execute_reply": "2022-12-06T02:50:00.343125Z"
        },
        "id": "77cfe326",
        "papermill": {
          "duration": 0.057854,
          "end_time": "2022-12-06T02:50:00.346565",
          "exception": false,
          "start_time": "2022-12-06T02:50:00.288711",
          "status": "completed"
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "class DownBlock(tf.keras.layers.Layer):\n",
        "    def __init__(\n",
        "        self, channels: int, channel_factor: float, *args, **kwargs\n",
        "    ):\n",
        "        super(DownBlock, self).__init__(*args, **kwargs)\n",
        "        self.average_pool = tf.keras.layers.AveragePooling2D(\n",
        "            pool_size=2, strides=2\n",
        "        )\n",
        "        self.conv = tf.keras.layers.Conv2D(\n",
        "            int(channels * channel_factor),\n",
        "            kernel_size=1,\n",
        "            strides=1,\n",
        "            padding=\"same\"\n",
        "        )\n",
        "\n",
        "    def call(self, inputs, *args, **kwargs):\n",
        "        return self.conv(self.average_pool(inputs))\n",
        "\n",
        "\n",
        "class DownSampleBlock(tf.keras.layers.Layer):\n",
        "    def __init__(\n",
        "        self,\n",
        "        channels: int,\n",
        "        scale_factor: int,\n",
        "        channel_factor: float,\n",
        "        *args,\n",
        "        **kwargs\n",
        "    ):\n",
        "        super(DownSampleBlock, self).__init__(*args, **kwargs)\n",
        "        self.layers = []\n",
        "        for _ in range(int(np.log2(scale_factor))):\n",
        "            self.layers.append(DownBlock(channels, channel_factor))\n",
        "            channels = int(channels * channel_factor)\n",
        "\n",
        "    def call(self, x, *args, **kwargs):\n",
        "        for layer in self.layers:\n",
        "            x = layer(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HWQzvMuilHHF"
      },
      "outputs": [],
      "source": [
        "class UpBlock(tf.keras.layers.Layer):\n",
        "    def __init__(self, channels: int, channel_factor: float, *args, **kwargs):\n",
        "        super().__init__(*args, **kwargs)\n",
        "        self.conv = tf.keras.layers.Conv2D(\n",
        "            int(channels // channel_factor), kernel_size=1, strides=1, padding=\"same\"\n",
        "        )\n",
        "        self.upsample = tf.keras.layers.UpSampling2D(size=2, interpolation=\"bilinear\")\n",
        "\n",
        "    def call(self, inputs, *args, **kwargs):\n",
        "        return self.upsample(self.conv(inputs))\n",
        "\n",
        "\n",
        "class UpSampleBlock(tf.keras.layers.Layer):\n",
        "    def __init__(\n",
        "        self, channels: int, scale_factor: int, channel_factor: float, *args, **kwargs\n",
        "    ):\n",
        "        super().__init__(*args, **kwargs)\n",
        "        self.layers = []\n",
        "        for _ in range(int(np.log2(scale_factor))):\n",
        "            self.layers.append(UpBlock(channels, channel_factor))\n",
        "            channels = int(channels // channel_factor)\n",
        "\n",
        "    def call(self, x, *args, **kwargs):\n",
        "        for layer in self.layers:\n",
        "            x = layer(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5xbxlGHFlMm_"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-12-06T02:50:00.523405Z",
          "iopub.status.busy": "2022-12-06T02:50:00.522832Z",
          "iopub.status.idle": "2022-12-06T02:50:00.559399Z",
          "shell.execute_reply": "2022-12-06T02:50:00.558365Z"
        },
        "id": "3551a758",
        "papermill": {
          "duration": 0.077697,
          "end_time": "2022-12-06T02:50:00.562014",
          "exception": false,
          "start_time": "2022-12-06T02:50:00.484317",
          "status": "completed"
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "class MultiScaleResidualBlock(tf.keras.layers.Layer):\n",
        "    def __init__(\n",
        "        self,\n",
        "        channels: int,\n",
        "        channel_factor: float,\n",
        "        groups: int,\n",
        "        *args,\n",
        "        **kwargs\n",
        "    ):\n",
        "        super().__init__(*args, **kwargs)\n",
        "\n",
        "        # Residual Context Blocks\n",
        "        self.rcb_top = ResidualContextBlock(\n",
        "            int(channels * channel_factor**0), groups=groups\n",
        "        )\n",
        "        self.rcb_middle = ResidualContextBlock(\n",
        "            int(channels * channel_factor**1), groups=groups\n",
        "        )\n",
        "        self.rcb_bottom = ResidualContextBlock(\n",
        "            int(channels * channel_factor**2), groups=groups\n",
        "        )\n",
        "\n",
        "        # Downsample Blocks\n",
        "        self.down_2 = DownSampleBlock(\n",
        "            channels=int((channel_factor**0) * channels),\n",
        "            scale_factor=2,\n",
        "            channel_factor=channel_factor,\n",
        "        )\n",
        "        self.down_4_1 = DownSampleBlock(\n",
        "            channels=int((channel_factor ** 0) * channels),\n",
        "            scale_factor=2,\n",
        "            channel_factor=channel_factor,\n",
        "        )\n",
        "        self.down_4_2 = DownSampleBlock(\n",
        "            channels=int((channel_factor ** 1) * channels),\n",
        "            scale_factor=2,\n",
        "            channel_factor=channel_factor,\n",
        "        )\n",
        "\n",
        "        # UpSample Blocks\n",
        "        self.up21_1 = UpSampleBlock(\n",
        "            channels=int((channel_factor ** 1) * channels),\n",
        "            scale_factor=2,\n",
        "            channel_factor=channel_factor,\n",
        "        )\n",
        "        self.up21_2 = UpSampleBlock(\n",
        "            channels=int((channel_factor ** 1) * channels),\n",
        "            scale_factor=2,\n",
        "            channel_factor=channel_factor,\n",
        "        )\n",
        "        self.up32_1 = UpSampleBlock(\n",
        "            channels=int((channel_factor ** 2) * channels),\n",
        "            scale_factor=2,\n",
        "            channel_factor=channel_factor,\n",
        "        )\n",
        "        self.up32_2 = UpSampleBlock(\n",
        "            channels=int((channel_factor ** 2) * channels),\n",
        "            scale_factor=2,\n",
        "            channel_factor=channel_factor,\n",
        "        )\n",
        "\n",
        "        # SKFF Blocks\n",
        "        self.skff_top = SelectiveKernelFeatureFusion(\n",
        "            channels=int(channels * channel_factor**0)\n",
        "        )\n",
        "        self.skff_middle = SelectiveKernelFeatureFusion(\n",
        "            channels=int(channels * channel_factor**1)\n",
        "        )\n",
        "\n",
        "        # Convolution\n",
        "        self.conv_out = tf.keras.layers.Conv2D(\n",
        "            channels, kernel_size=1, padding=\"same\"\n",
        "        )\n",
        "\n",
        "    def call(self, inputs, *args, **kwargs):\n",
        "        x_top = inputs\n",
        "        x_middle = self.down_2(x_top)\n",
        "        x_bottom = self.down_4_2(self.down_4_1(x_top))\n",
        "\n",
        "        x_top = self.rcb_top(x_top)\n",
        "        x_middle = self.rcb_middle(x_middle)\n",
        "        x_bottom = self.rcb_bottom(x_bottom)\n",
        "\n",
        "        x_middle = self.skff_middle([x_middle, self.up32_1(x_bottom)])\n",
        "        x_top = self.skff_top([x_top, self.up21_1(x_middle)])\n",
        "\n",
        "        x_top = self.rcb_top(x_top)\n",
        "        x_middle = self.rcb_middle(x_middle)\n",
        "        x_bottom = self.rcb_bottom(x_bottom)\n",
        "\n",
        "        x_middle = self.skff_middle([x_middle, self.up32_2(x_bottom)])\n",
        "        x_top = self.skff_top([x_top, self.up21_2(x_middle)])\n",
        "\n",
        "        output = self.conv_out(x_top)\n",
        "        output = output + inputs\n",
        "\n",
        "        return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nzc7opY8lOb7"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-12-06T02:50:00.731837Z",
          "iopub.status.busy": "2022-12-06T02:50:00.731425Z",
          "iopub.status.idle": "2022-12-06T02:50:00.748373Z",
          "shell.execute_reply": "2022-12-06T02:50:00.747468Z"
        },
        "id": "d4d839d4",
        "papermill": {
          "duration": 0.063879,
          "end_time": "2022-12-06T02:50:00.750942",
          "exception": false,
          "start_time": "2022-12-06T02:50:00.687063",
          "status": "completed"
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "class RecursiveResidualGroup(tf.keras.layers.Layer):\n",
        "    def __init__(\n",
        "        self,\n",
        "        channels: int,\n",
        "        num_mrb_blocks: int,\n",
        "        channel_factor: float,\n",
        "        groups: int,\n",
        "        *args,\n",
        "        **kwargs\n",
        "    ):\n",
        "        super().__init__(*args, **kwargs)\n",
        "        self.layers = [\n",
        "            MultiScaleResidualBlock(channels, channel_factor, groups)\n",
        "            for _ in range(num_mrb_blocks)\n",
        "        ]\n",
        "        self.layers.append(\n",
        "            tf.keras.layers.Conv2D(\n",
        "                channels, kernel_size=3, strides=1, padding=\"same\"\n",
        "            )\n",
        "        )\n",
        "\n",
        "    def call(self, inputs, *args, **kwargs):\n",
        "        residual = inputs\n",
        "        for layer in self.layers:\n",
        "            residual = layer(residual)\n",
        "        residual = residual + inputs\n",
        "        return residual\n",
        "\n",
        "\n",
        "class MirNetv2(tf.keras.Model):\n",
        "    def __init__(\n",
        "        self,\n",
        "        channels: int,\n",
        "        channel_factor: float,\n",
        "        num_mrb_blocks: int,\n",
        "        add_residual_connection: bool,\n",
        "        *args,\n",
        "        **kwargs\n",
        "    ):\n",
        "        super().__init__(*args, **kwargs)\n",
        "\n",
        "        self.add_residual_connection = add_residual_connection\n",
        "\n",
        "        self.conv_in = tf.keras.layers.Conv2D(\n",
        "            channels, kernel_size=3, padding=\"same\"\n",
        "        )\n",
        "\n",
        "        self.rrg_block_1 = RecursiveResidualGroup(\n",
        "            channels, num_mrb_blocks, channel_factor, groups=1\n",
        "        )\n",
        "        self.rrg_block_2 = RecursiveResidualGroup(\n",
        "            channels, num_mrb_blocks, channel_factor, groups=2\n",
        "        )\n",
        "        self.rrg_block_3 = RecursiveResidualGroup(\n",
        "            channels, num_mrb_blocks, channel_factor, groups=4\n",
        "        )\n",
        "        self.rrg_block_4 = RecursiveResidualGroup(\n",
        "            channels, num_mrb_blocks, channel_factor, groups=4\n",
        "        )\n",
        "\n",
        "        self.conv_out = tf.keras.layers.Conv2D(\n",
        "            3, kernel_size=3, padding=\"same\"\n",
        "        )\n",
        "\n",
        "    def call(self, inputs, training=None, mask=None):\n",
        "        shallow_features = self.conv_in(inputs)\n",
        "        deep_features = self.rrg_block_1(shallow_features)\n",
        "        deep_features = self.rrg_block_2(deep_features)\n",
        "        deep_features = self.rrg_block_3(deep_features)\n",
        "        deep_features = self.rrg_block_4(deep_features)\n",
        "        output = self.conv_out(deep_features)\n",
        "        output = output + inputs if self.add_residual_connection else output\n",
        "        return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wIqlJLQKlQwf"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-12-06T02:50:00.920558Z",
          "iopub.status.busy": "2022-12-06T02:50:00.920081Z",
          "iopub.status.idle": "2022-12-06T02:50:00.937274Z",
          "shell.execute_reply": "2022-12-06T02:50:00.936325Z"
        },
        "id": "5c0df6bd",
        "papermill": {
          "duration": 0.06319,
          "end_time": "2022-12-06T02:50:00.939867",
          "exception": false,
          "start_time": "2022-12-06T02:50:00.876677",
          "status": "completed"
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "class CharbonnierLoss(tf.keras.losses.Loss):\n",
        "    def __init__(self, epsilon: float, *args, **kwargs):\n",
        "        super().__init__(*args, **kwargs)\n",
        "        self.epsilon = tf.convert_to_tensor(epsilon)\n",
        "\n",
        "    def call(self, y_true, y_pred):\n",
        "        squared_difference = tf.square(y_true - y_pred)\n",
        "        return tf.reduce_mean(\n",
        "            tf.sqrt(squared_difference + tf.square(self.epsilon))\n",
        "        )\n",
        "\n",
        "\n",
        "class PSNRMetric(tf.keras.metrics.Metric):\n",
        "    def __init__(self, max_val: float, *args, **kwargs):\n",
        "        super().__init__(*args, **kwargs)\n",
        "        self.max_val = max_val\n",
        "        self.psnr = tf.keras.metrics.Mean(name=\"psnr\")\n",
        "\n",
        "    def update_state(self, y_true, y_pred, *args, **kwargs):\n",
        "        psnr = tf.image.psnr(y_true, y_pred, max_val=self.max_val)\n",
        "        self.psnr.update_state(psnr, *args, **kwargs)\n",
        "\n",
        "    def result(self):\n",
        "        return self.psnr.result()\n",
        "\n",
        "    def reset_state(self):\n",
        "        self.psnr.reset_state()\n",
        "\n",
        "\n",
        "class SSIMMetric(tf.keras.metrics.Metric):\n",
        "    def __init__(self, max_val: float, *args, **kwargs):\n",
        "        super().__init__(*args, **kwargs)\n",
        "        self.max_val = max_val\n",
        "        self.ssim = tf.keras.metrics.Mean(name=\"ssim\")\n",
        "\n",
        "    def update_state(self, y_true, y_pred, *args, **kwargs):\n",
        "        ssim = tf.image.ssim(y_true, y_pred, max_val=self.max_val)\n",
        "        self.ssim.update_state(ssim, *args, **kwargs)\n",
        "\n",
        "    def result(self):\n",
        "        return self.ssim.result()\n",
        "\n",
        "    def reset_state(self):\n",
        "        self.ssim.reset_state()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9ouGhAGPlS7e"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-12-06T02:50:01.105568Z",
          "iopub.status.busy": "2022-12-06T02:50:01.105142Z",
          "iopub.status.idle": "2022-12-06T02:50:15.880090Z",
          "shell.execute_reply": "2022-12-06T02:50:15.879079Z"
        },
        "id": "2a066f63",
        "papermill": {
          "duration": 14.819386,
          "end_time": "2022-12-06T02:50:15.882915",
          "exception": false,
          "start_time": "2022-12-06T02:50:01.063529",
          "status": "completed"
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "with strategy.scope():\n",
        "    model = MirNetv2(\n",
        "        channels=config.channels,\n",
        "        channel_factor=config.channel_factor,\n",
        "        num_mrb_blocks=config.num_mrb_blocks,\n",
        "        add_residual_connection=config.add_residual_connection\n",
        "    )\n",
        "\n",
        "    dummy_inputs = tf.ones((1, config.image_size, config.image_size, 3))\n",
        "    dummy_outputs = model(dummy_inputs)\n",
        "    model.summary(expand_nested=True)\n",
        "    print(\"\\nInput Shape:\", dummy_inputs.shape)\n",
        "    print(\"Output Shape:\", dummy_outputs.shape)\n",
        "\n",
        "    loss = CharbonnierLoss(epsilon=1e-3)\n",
        "\n",
        "    psnr_metric = PSNRMetric(max_val=1.0)\n",
        "    ssim_metric = SSIMMetric(max_val=1.0)\n",
        "\n",
        "    decay_steps = (config.max_train_images // config.batch_size) * config.epochs\n",
        "    lr_schedule_fn = tf.keras.optimizers.schedules.CosineDecay(\n",
        "        initial_learning_rate=config.initial_learning_rate,\n",
        "        decay_steps=decay_steps,\n",
        "        alpha=config.minimum_learning_rate,\n",
        "    )\n",
        "    optimizer = tf.keras.optimizers.Adam(\n",
        "        learning_rate=lr_schedule_fn, beta_1=0.9, beta_2=0.999,\n",
        "    )\n",
        "\n",
        "    model.compile(\n",
        "        optimizer=optimizer, loss=loss, metrics=[psnr_metric, ssim_metric]\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v7dxkqbGlU0T"
      },
      "outputs": [],
      "source": [
        "%%wandb\n",
        "\n",
        "callbacks = [\n",
        "    WandbMetricsLogger(),\n",
        "    WandbModelCheckpoint(filepath=\"model\", save_best_only=False)\n",
        "]\n",
        "\n",
        "model.fit(\n",
        "    train_dataset,\n",
        "    validation_data=val_dataset,\n",
        "    epochs=config.epochs,\n",
        "    callbacks=callbacks\n",
        ")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
